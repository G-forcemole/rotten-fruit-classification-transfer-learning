{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fruit Freshness Classification Project\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "The goal of this project is to build a **multi-class image classification system** capable of distinguishing between **fresh and rotten fruit** across multiple categories. This problem mirrors real-world quality control and food inspection use cases, where automated visual systems are used to detect spoilage and ensure product quality.\n",
    "\n",
    "The model is trained to classify images into **six categories**:\n",
    "- Fresh apples  \n",
    "- Fresh oranges  \n",
    "- Fresh bananas  \n",
    "- Rotten apples  \n",
    "- Rotten oranges  \n",
    "- Rotten bananas  \n",
    "\n",
    "Due to the relatively limited size of the dataset, this project emphasizes the use of **transfer learning**, **data augmentation**, and **fine-tuning** to achieve strong generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Approach\n",
    "\n",
    "To solve this problem, I leveraged a **pretrained convolutional neural network** as a feature extractor and added a custom classification head suited for the six target classes. The training pipeline includes:\n",
    "\n",
    "- Transfer learning using a pretrained image backbone\n",
    "- Data augmentation to improve robustness and reduce overfitting\n",
    "- Fine-tuning with a reduced learning rate once the classifier converges\n",
    "- Categorical cross-entropy loss for multi-class classification\n",
    "- Validation-based evaluation to monitor generalization performance\n",
    "\n",
    "The model is considered successful once it reaches strong validation accuracy while maintaining stable loss behavior, demonstrating its ability to generalize beyond the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset is organized under the `data/fruits/` directory and follows a standard image classification folder structure. Images are grouped by class, allowing labels to be inferred directly from directory names during training.\n",
    "\n",
    "This structure enables efficient loading using a custom PyTorch `Dataset` and supports scalable experimentation with different model architectures and preprocessing pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Below, I define the training environment, data loaders, and model architecture using **PyTorch** and **Torchvision**. The training loop tracks both loss and accuracy across training and validation sets to ensure the model learns meaningful visual representations of fruit freshness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This project focuses on building a multi-class image classification system to distinguish between **fresh and rotten fruits** using computer vision and deep learning.\n",
    "\n",
    "The dataset consists of six classes:\n",
    "- Fresh apples\n",
    "- Fresh bananas\n",
    "- Fresh oranges\n",
    "- Rotten apples\n",
    "- Rotten bananas\n",
    "- Rotten oranges\n",
    "\n",
    "Images are organized in a directory-based structure (`data/fruits/`), enabling label inference directly from folder names. This structure supports scalable dataset loading and aligns with standard computer vision workflows.\n",
    "\n",
    "Given the six target categories, the model architecture is designed with a **six-unit output layer**, optimized using **categorical cross-entropy loss** to handle multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with ImageNet\n",
    "\n",
    "To improve performance on a limited dataset, this project leverages **transfer learning** using a convolutional neural network pretrained on **ImageNet**.\n",
    "\n",
    "A VGG16 architecture is used as the base feature extractor. Pretrained weights provide robust low- and mid-level visual features (edges, textures, shapes), allowing the model to generalize effectively despite limited labeled data.\n",
    "\n",
    "Since the dataset consists of RGB images, the model operates on three-channel inputs. The pretrained backbone is later extended with custom classification layers tailored to the six fruit categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to start with a model pretrained on ImageNet. Load the model with the correct weights. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. If you need a reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where we implemented transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preserve the general visual features learned from ImageNet, the pretrained VGG16 backbone is **frozen during initial training**. This prevents large gradient updates from overwriting useful low-level representations such as edges, textures, and shapes.\n",
    "\n",
    "Freezing the base model allows training to focus on the newly added classification layers, which adapt the pretrained features to the fruit freshness task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Classification Head\n",
    "\n",
    "The pretrained VGG16 model is extended with a **custom classification head** tailored to the fruit freshness dataset.\n",
    "\n",
    "Early layers of VGG16 capture general visual patterns, while deeper layers become increasingly task-specific. To balance generalization and task adaptation, only a subset of the original classifier layers is retained.\n",
    "\n",
    "Additional fully connected layers are appended to:\n",
    "- Reduce feature dimensionality\n",
    "- Introduce non-linearity\n",
    "- Produce class logits for six fruit categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.classifier[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (4): Linear(in_features=4096, out_features=500, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration and Optimization\n",
    "\n",
    "The model is configured for **multi-class classification** using a categorical loss function appropriate for six mutually exclusive classes.\n",
    "\n",
    "The Adam optimizer is selected for its adaptive learning rate and strong performance on deep convolutional networks. All model parameters are moved to the selected compute device prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = torch.compile(my_model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the input images, we will use the transforms included with the VGG16 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is relatively small, I apply light, controlled data augmentation\n",
    "during training to improve generalization while preserving the semantic structure\n",
    "of the images. Augmentations are intentionally kept subtle to avoid introducing\n",
    "unrealistic artifacts that could harm classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than relying on a prebuilt dataset loader, I implemented a custom `Dataset` class to maintain full control over image preprocessing, labeling logic, and device placement. Images are read directly from disk, converted to RGB, and mapped to integer class labels based on folder names, enabling a clean and extensible data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"] \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "\n",
    "train_path = \"data/fruits/train/\"\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_path = \"data/fruits/valid/\"\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "I trained the model using a custom training loop that separates training and validation logic for clarity and reuse. To keep the notebook focused on experimentation, the core `train` and `validate` routines are implemented in a separate utility module. (utils.py)\n",
    "\n",
    "Training was performed for a fixed number of epochs while monitoring both training loss and validation accuracy to ensure stable convergence and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 13.2651 Accuracy: 0.8748\n",
      "Valid - Loss: 2.8015 Accuracy: 0.9453\n",
      "Epoch: 1\n",
      "Train - Loss: 4.9151 Accuracy: 0.9569\n",
      "Valid - Loss: 1.3459 Accuracy: 0.9666\n",
      "Epoch: 2\n",
      "Train - Loss: 2.3922 Accuracy: 0.9788\n",
      "Valid - Loss: 2.0203 Accuracy: 0.9605\n",
      "Epoch: 3\n",
      "Train - Loss: 2.5682 Accuracy: 0.9755\n",
      "Valid - Loss: 2.0580 Accuracy: 0.9514\n",
      "Epoch: 4\n",
      "Train - Loss: 2.9935 Accuracy: 0.9695\n",
      "Valid - Loss: 1.0286 Accuracy: 0.9605\n",
      "Epoch: 5\n",
      "Train - Loss: 1.9643 Accuracy: 0.9839\n",
      "Valid - Loss: 1.4339 Accuracy: 0.9635\n",
      "Epoch: 6\n",
      "Train - Loss: 3.2611 Accuracy: 0.9721\n",
      "Valid - Loss: 1.3228 Accuracy: 0.9726\n",
      "Epoch: 7\n",
      "Train - Loss: 1.8676 Accuracy: 0.9839\n",
      "Valid - Loss: 1.4661 Accuracy: 0.9574\n",
      "Epoch: 8\n",
      "Train - Loss: 2.3329 Accuracy: 0.9738\n",
      "Valid - Loss: 1.0605 Accuracy: 0.9666\n",
      "Epoch: 9\n",
      "Train - Loss: 1.5466 Accuracy: 0.9873\n",
      "Valid - Loss: 1.3923 Accuracy: 0.9605\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Pretrained Backbone\n",
    "\n",
    "After achieving strong validation performance with a frozen ImageNet backbone, I performed a brief fine-tuning step by unfreezing the pretrained layers and continuing training with a very low learning rate.\n",
    "\n",
    "This allows the deeper convolutional features to adapt slightly to domain-specific visual cues (fresh vs. rotten fruit) while minimizing the risk of catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 1.2777 Accuracy: 0.9856\n",
      "Valid - Loss: 1.3674 Accuracy: 0.9574\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Conclusion\n",
    "\n",
    "In this project, I developed an end-to-end image classification system to distinguish between **fresh and rotten fruit across six categories** using transfer learning with a pretrained VGG16 backbone.\n",
    "\n",
    "Starting from raw image data, I designed a custom PyTorch dataset pipeline, applied task-appropriate data augmentation, and leveraged pretrained ImageNet features to accelerate convergence and improve generalization. By initially freezing the backbone and training a lightweight classifier head, the model achieved strong baseline performance. A brief fine-tuning phase with a low learning rate further refined high-level visual features while preserving pretrained representations.\n",
    "\n",
    "The final model achieved **over 95% validation accuracy**, demonstrating robust performance across multiple fruit types and spoilage conditions. This project highlights the effectiveness of transfer learning for small-to-medium image datasets and reinforces best practices in model freezing, controlled fine-tuning, and GPU-efficient training.\n",
    "\n",
    "Future improvements could include experimenting with alternative architectures (e.g., ResNet or EfficientNet), incorporating class-imbalance handling, or extending the model to real-world deployment scenarios such as mobile inference or edge devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
